import requests
from bs4 import BeautifulSoup
import json
import time
import os
import re

# 시작 페이지 URL
start_page_url = 'https://www.guide-to-houseplants.com/house-plants-encyclopedia-a-z.html'

# 저장할 데이터를 담을 리스트
data_list = []

def fetch_url(url, retries=3, delay=5):
    """지정된 URL에서 콘텐츠를 가져옴. 실패 시 재시도."""
    for i in range(retries):
        try:
            response = requests.get(url, timeout=10)
            if response.status_code == 200:
                return response
        except requests.exceptions.RequestException as e:
            print(f"Error fetching {url}: {e}")
            time.sleep(delay)
    return None

def extract_text_below_headers(soup):
    """h2 및 h3 태그 아래의 모든 p 태그의 텍스트를 추출"""
    extracted_text = ""
    
    for header in soup.find_all(['h2', 'h3']):
        header_text = header.get_text()
        if 'Get to Know' in header_text or 'Care' in header_text:
            # div 태그 중 class가 WhatsNew인 경우 건너뛰기
            if header.find_parent('div', class_='WhatsNew'):
                continue
            extracted_text += header_text + "\n"
            next_sibling = header.find_next_sibling()
            while next_sibling:
                if next_sibling.name == 'p':
                    extracted_text += next_sibling.get_text() + "\n"
                elif next_sibling.name in ['h1', 'h2', 'h3', 'h4', 'h5', 'h6']:
                    break
                next_sibling = next_sibling.find_next_sibling()
    
    return extracted_text.strip()

def remove_html_sentences(text):
    """텍스트에서 .html을 포함하는 문장을 제거"""
    sentences = text.split('\n')
    cleaned_sentences = [sentence for sentence in sentences if '.html' not in sentence]
    return '\n'.join(cleaned_sentences)

def remove_duplicate_sentences(text):
    """중복된 문장이 있으면 앞에 있는 문장을 제거"""
    sentences = text.split('\n')
    seen = set()
    cleaned_sentences = []
    for sentence in reversed(sentences):  # 뒤에서부터 처리
        if sentence not in seen:
            seen.add(sentence)
            cleaned_sentences.append(sentence)
    return '\n'.join(reversed(cleaned_sentences))

def remove_characters(text, characters):
    """텍스트에서 특정 문자를 제거"""
    for char in characters:
        text = text.replace(char, '')
    return text

# 제거할 문자들 예시
characters_to_remove = ['\n']

# 시작 페이지에서 HTML 가져오기
response = fetch_url(start_page_url)
if response:
    soup = BeautifulSoup(response.content, 'html.parser')

    # 시작 페이지에서 사이트 주소 추출
    for link in soup.find_all('a', href=True):
        subpage_url = link['href']
        if subpage_url.startswith('https://www.guide-to-houseplants.com/'):
            # 서브 페이지로 이동하여 HTML 가져오기
            subpage_response = fetch_url(subpage_url)
            if not subpage_response:
                print(f"Failed to fetch {subpage_url} after retries. Skipping.")
                continue
            
            subpage_soup = BeautifulSoup(subpage_response.content, 'html.parser')

            # h2 및 h3 태그 아래의 모든 p 태그 텍스트 추출
            extracted_text = extract_text_below_headers(subpage_soup)

            # 첫 번째 실제 이미지 파일의 data-src 추출
            first_image = subpage_soup.find('img', {'data-src': True})
            if first_image:
                image_data_src = first_image['data-src']
            else:
                image_data_src = "Image URL Not Found"

            # .html이 포함된 문장 제거
            cleaned_text = remove_html_sentences(extracted_text.strip())

            # 중복된 문장 제거
            cleaned_text = remove_duplicate_sentences(cleaned_text)

            # 특정 문자를 제거
            cleaned_text = remove_characters(cleaned_text, characters_to_remove)

            # 데이터가 존재하면 리스트에 추가
            if cleaned_text:
                data_entry = {
                    "url": image_data_src,
                    "content": cleaned_text
                }
                data_list.append(data_entry)
                
                # 실시간으로 URL과 추출된 내용 출력
                print("Image URL: {}".format(image_data_src))
                print("Extracted Content: {}".format(cleaned_text))
                print("-" * 50)
else:
    print(f"Failed to fetch the start page: {start_page_url}")

# JSON 파일로 저장
output_file = 'extracted_data4.json'
with open(output_file, 'w', encoding='utf-8') as json_file:
    json.dump(data_list, json_file, ensure_ascii=False, indent=4)

# 파일 저장 여부 및 위치 확인
if os.path.exists(output_file):
    print(f"Data extraction and saving to JSON file completed. File saved as {output_file}")
    print(f"Full path: {os.path.abspath(output_file)}")
else:
    print("Failed to save the JSON file.")
